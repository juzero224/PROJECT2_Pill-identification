{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T09:40:23.864768Z","iopub.status.busy":"2022-08-18T09:40:23.864388Z","iopub.status.idle":"2022-08-18T09:40:39.989804Z","shell.execute_reply":"2022-08-18T09:40:39.988226Z","shell.execute_reply.started":"2022-08-18T09:40:23.864734Z"},"id":"m1_Lh4OHe_Al","trusted":true},"outputs":[],"source":["# Download TorchVision repo to use some files from\n","# references/detection\n","!pip install pycocotools --quiet\n","!git clone https://github.com/pytorch/vision.git\n","!git checkout v0.3.0\n","\n","!cp vision/references/detection/utils.py ./\n","!cp vision/references/detection/transforms.py ./\n","!cp vision/references/detection/coco_eval.py ./\n","!cp vision/references/detection/engine.py ./\n","!cp ../input/please-run/coco_utils.py ./"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUm6qo6ne_w7"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T09:40:42.174587Z","iopub.status.busy":"2022-08-18T09:40:42.174203Z","iopub.status.idle":"2022-08-18T09:40:42.181372Z","shell.execute_reply":"2022-08-18T09:40:42.179965Z","shell.execute_reply.started":"2022-08-18T09:40:42.174547Z"},"id":"XVND0ECHfB4K","trusted":true},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # cuda 관련 오류 자세하게 알려주는 코드 (코랩용)\n","# torch 모듈 전에 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T09:40:43.716884Z","iopub.status.busy":"2022-08-18T09:40:43.716283Z","iopub.status.idle":"2022-08-18T09:40:43.723810Z","shell.execute_reply":"2022-08-18T09:40:43.722762Z","shell.execute_reply.started":"2022-08-18T09:40:43.716845Z"},"id":"0jDpqxu1fDSR","trusted":true},"outputs":[],"source":["import torch\n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T09:40:47.095800Z","iopub.status.busy":"2022-08-18T09:40:47.094679Z","iopub.status.idle":"2022-08-18T09:40:47.103806Z","shell.execute_reply":"2022-08-18T09:40:47.102663Z","shell.execute_reply.started":"2022-08-18T09:40:47.095754Z"},"id":"QJY6W1EDfHJJ","trusted":true},"outputs":[],"source":["# Basic python and ML Libraries\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","# for ignoring warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# We will be reading images using OpenCV\n","import cv2\n","\n","# xml library for parsing xml files\n","from xml.etree import ElementTree as et\n","\n","# matplotlib for visualization\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","# torchvision libraries\n","import torch\n","import torchvision\n","from torchvision import transforms as torchtrans  \n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","# these are the helper libraries imported.\n","from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","\n","# for image augmentations\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import random\n","import shutil"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fa2du_6-IRvD"},"outputs":[],"source":["# a = [1,2,3,4,5]\n","# a[1:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T10:01:04.510871Z","iopub.status.busy":"2022-08-18T10:01:04.510474Z","iopub.status.idle":"2022-08-18T10:01:57.184601Z","shell.execute_reply":"2022-08-18T10:01:57.183356Z","shell.execute_reply.started":"2022-08-18T10:01:04.510838Z"},"id":"1_xT79EnU5as","trusted":true},"outputs":[],"source":["# print(len(os.listdir('/content/drive/MyDrive/그게뭐약/alayc_img/alayc_img')))  # image file\n","# print(len(os.listdir('/content/drive/MyDrive/그게뭐약/alayc_xml/alayc_xml')))  # xml file\n","# !mkdir alyac_img\n","# !mkdir alyac_xml\n","\n","# !unzip -qq '/content/drive/MyDrive/All_png.zip' -d '/content/alyac_img'\n","# !unzip -qq '/content/drive/MyDrive/All_xml.zip' -d '/content/alyac_xml'\n","\n","!mkdir test_images\n","!mkdir test_annotations\n","!mkdir images\n","!mkdir annotations\n","!mkdir files_dir\n","!mkdir test_dir\n","\n","random.seed(1234)\n","idx = random.sample(range(24000), 2400)  # 8:2\n","\n","for img in np.array(sorted(os.listdir('../input/please-run/All_png'))):\n","    shutil.copy('../input/please-run/All_png/'+img, 'images/'+img)\n","\n","for annot in np.array(sorted(os.listdir('../input/please-run/All_xml'))):\n","    shutil.copy('../input/please-run/All_xml/'+annot, 'annotations/'+annot)\n","\n","for img in np.array(sorted(os.listdir('../input/please-run/All_png')))[idx]:\n","    shutil.move('images/'+img, 'test_images/'+img)\n","\n","for annot in np.array(sorted(os.listdir('../input/please-run/All_xml')))[idx]:\n","    shutil.move('annotations/'+annot, 'test_annotations/'+annot)\n","\n","for files in sorted(os.listdir('images')):\n","    shutil.move('images/'+files, 'files_dir/'+files)\n","\n","for files in sorted(os.listdir('annotations')):\n","    shutil.move('annotations/'+files, 'files_dir/'+files)\n","\n","for files in sorted(os.listdir('test_images')):\n","    shutil.move('test_images/'+files, 'test_dir/'+files)\n","\n","for files in sorted(os.listdir('test_annotations')):\n","    shutil.move('test_annotations/'+files, 'test_dir/'+files)\n","\n","print(len(os.listdir('annotations')))\n","print(len(os.listdir('images')))\n","print(len(os.listdir('test_annotations')))\n","print(len(os.listdir('test_images')))\n","print(len(os.listdir('files_dir')))\n","print(len(os.listdir('test_dir')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzqIUg_vfI7h"},"outputs":[],"source":["# print(len(os.listdir('/content/drive/MyDrive/alyac_ALL')))  # image file\n","\n","# !mkdir files_dir\n","# !mkdir test_dir\n","\n","# for files in np.array(sorted(os.listdir('/content/drive/MyDrive/alyac_ALL'))):\n","#     shutil.copy('/content/drive/MyDrive/alyac_ALL/'+files, 'files_dir/'+files)\n","\n","# # # print(sorted(os.listdir('/content/files_dir')))\n","# # # random.seed(1234)\n","# # # idx = []\n","# # # for i in os.listdir('/content/files_dir'):\n","# # #   idx.append(i)\n","# # #   idx.sort()\n","# # # idx = idx[569:]\n","\n","\n","# # # for files in np.array(sorted(os.listdir('/content/drive/MyDrive/alyac_ALL'))):\n","# # #     shutil.copy('/content/drive/MyDrive/alyac_ALL/'+files, 'files_dir/'+files)\n","\n","# for files in sorted(os.listdir('/content/files_dir'))[569:]:\n","#     shutil.move('files_dir/'+files, 'test_dir/'+files)\n","\n","# # print(sorted(os.listdir('/content/test_dir')))\n","\n","# # # # shutil.copy(source, destination) \n","# # # # >> source : 소스 파일의 경로를 나타내는 문자열\n","# # # # >> destination : 대상 파일 또는 디렉토리의 경로를 나타내는 문자열\n","\n","# print(len(os.listdir('files_dir')))\n","# print(len(os.listdir('test_dir')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4CQVOEoMeOx"},"outputs":[],"source":["# os.listdir('test_dir')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T10:04:01.174918Z","iopub.status.busy":"2022-08-18T10:04:01.174505Z","iopub.status.idle":"2022-08-18T10:04:01.181676Z","shell.execute_reply":"2022-08-18T10:04:01.180606Z","shell.execute_reply.started":"2022-08-18T10:04:01.174884Z"},"id":"EgaBf5Er5Wbo","trusted":true},"outputs":[],"source":["files_dir = 'files_dir'\n","test_dir = 'test_dir'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T10:04:24.705110Z","iopub.status.busy":"2022-08-18T10:04:24.704724Z","iopub.status.idle":"2022-08-18T10:04:40.433563Z","shell.execute_reply":"2022-08-18T10:04:40.432681Z","shell.execute_reply.started":"2022-08-18T10:04:24.705078Z"},"id":"gY6NbXcEA02u","trusted":true},"outputs":[],"source":["from os import listdir\n","\n","def read_annotations(xml_file: str):\n","    tree = et.parse(xml_file)  # xml 파일 읽어오기\n","    root = tree.getroot()   # get root node   # xml 문서의 최상단 루트 태그\n","\n","    classes_list = []\n","\n","    file_name = root.find('filename').text    # root 하위에 'filename' 태그를 찾아서 text 읽기 (= find().text)\n","    for obj in root.iter('object'):    # root 태그에서 iter('object')만 순회.  # find는 해당 태그의 바로 자식만 탐색 / iter는 모든 자식에 대해 탐색\n","        object_label = obj.find(\"name\").text     # 이미지 name (=class)\n","\n","        classes = [object_label]\n","        # print(classes)\n","        classes_list.append(classes)\n","        # print(classes_list)\n","    return classes_list\n","\n","def get_classes(dir):\n","    classes = []\n","    for file in listdir(dir):\n","        if 'xml' in file.lower():\n","            classes_file = file.replace(file.split('.')[-1], 'xml')\n","            class_name = read_annotations(dir+classes_file)\n","            classes.append(class_name)\n","            # print(classes)\n","    return classes\n","\n","classes = get_classes('../input/please-run/All_xml/')\n","\n","class_list = []\n","for t in classes:\n","    if t not in class_list:\n","        class_list.append(t[0][0])\n","\n","class_list = pd.Series(class_list).unique()\n","class_list = class_list.tolist()\n","class_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqruP9ui6EBP"},"outputs":[],"source":["# import pandas as pd\n","\n","# class_list = []\n","# for t in os.listdir('/content/drive/MyDrive/alyac_ALL'):\n","#   t = t.replace(t.split('.')[-1], '')\n","#   t = t.replace('.','')\n","#   # if t not in class_list:\n","#   class_list.append(t)\n","#   # pd.Series(class_list).unique()\n","# # print(class_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9N7JE4-W-rS-"},"outputs":[],"source":["# import pandas as pd\n","\n","# class_list = []\n","# for t in os.listdir('/content/drive/MyDrive/alyac_ALL'):\n","#   class_list.append(t)\n","\n","# len(class_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T10:05:33.471066Z","iopub.status.busy":"2022-08-18T10:05:33.470679Z","iopub.status.idle":"2022-08-18T10:05:33.591510Z","shell.execute_reply":"2022-08-18T10:05:33.590620Z","shell.execute_reply.started":"2022-08-18T10:05:33.471033Z"},"id":"Q8w5s4xAfLfR","trusted":true},"outputs":[],"source":["class PillImagesDataset(torch.utils.data.Dataset):\n","  def __init__(self, files_dir, width, height, transforms=None):\n","    self.transforms = transforms\n","    self.files_dir = files_dir\n","    self.height = height\n","    self.width = width\n","\n","    # sorting the images for consistency\n","    self.imgs = [image for image in sorted(os.listdir(files_dir))\n","                    if image[-4:]=='.png']\n","    \n","    # classes : 0 index is reserved for background\n","    self.classes = class_list\n","\n","  def __getitem__(self, idx):\n","\n","    img_name = self.imgs[idx]\n","    image_path = os.path.join(self.files_dir, img_name)\n","\n","    img = cv2.imread(image_path)\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","    img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n","    # diving by 255\n","    img_res /= 255.0\n","\n","    # annotation file\n","    annot_filename = img_name[:-4] + '.xml'\n","    annot_file_path = os.path.join(self.files_dir, annot_filename)\n","    # print(annot_filename)\n","\n","    boxes = []\n","    labels = []\n","    tree = et.parse(annot_file_path)\n","    root = tree.getroot()\n","\n","    wt = img.shape[1]\n","    ht = img.shape[0]\n","\n","    for member in root.findall('object'):\n","      labels.append(self.classes.index(member.find('name').text))\n","\n","      # bounding box\n","      xmin = int(member.find('bndbox').find('xmin').text)\n","      xmax = int(member.find('bndbox').find('xmax').text)\n","\n","      ymin = int(member.find('bndbox').find('ymin').text)\n","      ymax = int(member.find('bndbox').find('ymax').text)\n","\n","      xmin_corr = (xmin/wt)*self.width\n","      xmax_corr = (xmax/wt)*self.width\n","      ymin_corr = (ymin/ht)*self.height\n","      ymax_corr = (ymax/ht)*self.height\n","\n","      boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n","\n","      # xmin = float(member.find('bndbox').find('xmin').text)\n","      # ymin = float(member.find('bndbox').find('ymin').text)\n","      # xmax = float(member.find('bndbox').find('xmax').text)\n","      # ymax = float(member.find('bndbox').find('ymax').text)\n","      # boxes.append([xmin, xmax, ymin, ymax])\n","    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","\n","    area = (boxes[:, 3] - boxes[:, 1])*(boxes[:, 2]-boxes[:, 0])\n","\n","    # suppose all instances are not crowd\n","    iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n","\n","    labels = torch.as_tensor(labels, dtype=torch.int64)\n","\n","    target = {}\n","    target['boxes'] = boxes\n","    target['labels'] = labels\n","    target['area'] = area\n","    target['iscrowed'] = iscrowd\n","\n","    image_id = torch.tensor([idx])\n","    target['image_id'] = image_id\n","\n","    if self.transforms:\n","\n","      sample = self.transforms(image = img_res,\n","                               bboxes = target['boxes'],\n","                               labels = labels)\n","      img_res = sample['image']\n","      target['boxes'] = torch.Tensor(sample['bboxes'])\n","\n","    return img_res, target\n","\n","  def __len__(self):\n","    return len(self.imgs)\n","\n","# check dataset\n","dataset = PillImagesDataset(files_dir, 150, 150)\n","print('length of dataset = ', len(dataset), '\\n')\n","\n","# getting the image and target for a test index. Feel free to change the index\n","img, target = dataset[78]\n","print(img.shape, '\\n', target)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:11:53.598686Z","iopub.status.busy":"2022-08-18T18:11:53.598311Z","iopub.status.idle":"2022-08-18T18:11:53.806479Z","shell.execute_reply":"2022-08-18T18:11:53.805259Z","shell.execute_reply.started":"2022-08-18T18:11:53.598654Z"},"id":"JKkoWx9kgjHa","trusted":true},"outputs":[],"source":["# Function to visualize bounding boxes in the image\n","\n","def plot_img_bbox(img, target, label):\n","    # plot the image and bboxes\n","    # Bounding boxes are defined as follows: x-min y-min width height\n","    fig, a = plt.subplots(1,1)\n","    fig.set_size_inches(5,5)\n","    a.imshow(img)\n","    \n","    for box in (target['boxes']):\n","        box = box.detach().cpu().numpy()\n","        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n","        rect = patches.Rectangle((x, y),\n","                                 width, height,\n","                                 linewidth = 2,\n","                                 edgecolor = 'r',\n","                                 facecolor = 'none')\n","\n","        # Draw the bounding box on top of the image\n","        a.add_patch(rect)\n","        if label is not None:\n","            label = target['labels']\n","            a.text(box[0]+0.5 * (box[2]-box[0]), box[1] + 0.5 * (box[3]-box[1]), label, ha='center', va='center', color='r')\n","    plt.show()\n","    \n","# plotting the image with bboxes. Feel free to change the index\n","img, target = dataset[25]\n","plot_img_bbox(img, target, _)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T10:05:42.604726Z","iopub.status.busy":"2022-08-18T10:05:42.604354Z","iopub.status.idle":"2022-08-18T10:05:42.610769Z","shell.execute_reply":"2022-08-18T10:05:42.609686Z","shell.execute_reply.started":"2022-08-18T10:05:42.604678Z"},"id":"YkIx4m7Eg2Gx","trusted":true},"outputs":[],"source":["def get_object_detection_model(num_classes):\n","\n","    # load a model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    \n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T10:05:45.546862Z","iopub.status.busy":"2022-08-18T10:05:45.546481Z","iopub.status.idle":"2022-08-18T10:05:45.552617Z","shell.execute_reply":"2022-08-18T10:05:45.551723Z","shell.execute_reply.started":"2022-08-18T10:05:45.546829Z"},"id":"Aiv6u5Vpg9jY","trusted":true},"outputs":[],"source":["# Send train=True fro training transforms and False for val/test transforms\n","def get_transform(train):\n","    \n","    if train:\n","        return A.Compose([\n","                            A.HorizontalFlip(0.5),\n","                     # ToTensorV2 converts image to pytorch tensor without div by 255\n","                            ToTensorV2(p=1.0) \n","                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","    else:\n","        return A.Compose([\n","                            ToTensorV2(p=1.0)\n","                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T10:06:05.259258Z","iopub.status.busy":"2022-08-18T10:06:05.258891Z","iopub.status.idle":"2022-08-18T10:06:05.418777Z","shell.execute_reply":"2022-08-18T10:06:05.417752Z","shell.execute_reply.started":"2022-08-18T10:06:05.259226Z"},"id":"cgpsgcvEhBGY","trusted":true},"outputs":[],"source":["# use our dataset and defined transformations\n","dataset = PillImagesDataset('files_dir', 480, 480, transforms= get_transform(train=True))\n","dataset_test = PillImagesDataset('files_dir', 480, 480, transforms= get_transform(train=False))\n","\n","# split the dataset in train and test set\n","torch.manual_seed(1)\n","indices = torch.randperm(len(dataset)).tolist()\n","\n","# train test split\n","test_split = 0.2\n","tsize = int(len(dataset)*test_split)\n","dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=16, shuffle=True, num_workers=4,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=8, shuffle=False, num_workers=4,\n","    collate_fn=utils.collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T10:06:12.452055Z","iopub.status.busy":"2022-08-18T10:06:12.451350Z","iopub.status.idle":"2022-08-18T10:06:32.793582Z","shell.execute_reply":"2022-08-18T10:06:32.792644Z","shell.execute_reply.started":"2022-08-18T10:06:12.452017Z"},"id":"ozJT1ghjhmII","trusted":true},"outputs":[],"source":["# to train on gpu if selected.\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","\n","num_classes = len(class_list)\n","\n","# get the model using our helper function\n","model = get_object_detection_model(num_classes)\n","\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3,\n","                                               gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T10:09:41.341652Z","iopub.status.busy":"2022-08-18T10:09:41.341126Z","iopub.status.idle":"2022-08-18T17:12:44.382879Z","shell.execute_reply":"2022-08-18T17:12:44.381737Z","shell.execute_reply.started":"2022-08-18T10:09:41.341606Z"},"id":"nUMcccALhp5w","trusted":true},"outputs":[],"source":["# training for 10 epochs\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    # training for one epoch\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    evaluate(model, data_loader_test, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:41:29.401919Z","iopub.status.busy":"2022-08-18T18:41:29.401436Z","iopub.status.idle":"2022-08-18T18:41:29.407075Z","shell.execute_reply":"2022-08-18T18:41:29.405970Z","shell.execute_reply.started":"2022-08-18T18:41:29.401881Z"},"id":"I5iCiwnrIr47","trusted":true},"outputs":[],"source":["def mAP(result):\n","    ap = 0\n","    for r in result:\n","        ap += r['AP']\n","    mAP = ap / len(result)\n","    \n","    return mAP"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T17:12:44.385840Z","iopub.status.busy":"2022-08-18T17:12:44.385431Z","iopub.status.idle":"2022-08-18T17:12:44.393462Z","shell.execute_reply":"2022-08-18T17:12:44.392408Z","shell.execute_reply.started":"2022-08-18T17:12:44.385802Z"},"id":"y-zsJ2GLhs1A","trusted":true},"outputs":[],"source":["# the function takes the original prediction and the iou threshold.\n","\n","def apply_nms(orig_prediction, iou_thresh=0.3):\n","    \n","    # torchvision returns the indices of the bboxes to keep\n","    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n","    \n","    final_prediction = orig_prediction\n","    final_prediction['boxes'] = final_prediction['boxes'][keep]\n","    final_prediction['scores'] = final_prediction['scores'][keep]\n","    final_prediction['labels'] = final_prediction['labels'][keep]\n","    \n","    return final_prediction\n","\n","# function to convert a torchtensor back to PIL image\n","def torch_to_pil(img):\n","    return torchtrans.ToPILImage()(img).convert('RGB')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T17:18:06.456649Z","iopub.status.busy":"2022-08-18T17:18:06.455957Z","iopub.status.idle":"2022-08-18T17:18:06.551748Z","shell.execute_reply":"2022-08-18T17:18:06.550588Z","shell.execute_reply.started":"2022-08-18T17:18:06.456610Z"},"id":"ydihnvxYuJrZ","trusted":true},"outputs":[],"source":["# # pick one image from the test set\n","# img, target = dataset_test[5]\n","# # put the model in evaluation mode\n","# model.eval()\n","# with torch.no_grad():\n","#     prediction = model([img.to(device)])[0]\n","    \n","# print('predicted #boxes: ', len(prediction['labels']))\n","# print('real #boxes: ', len(target['labels']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGeZkVRhIr47"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T19:33:42.741642Z","iopub.status.busy":"2022-08-18T19:33:42.741262Z","iopub.status.idle":"2022-08-18T19:33:42.751536Z","shell.execute_reply":"2022-08-18T19:33:42.750474Z","shell.execute_reply.started":"2022-08-18T19:33:42.741611Z"},"id":"EgoI3qNgIr48","trusted":true},"outputs":[],"source":["def make_prediction(model, img, threshold):\n","    model.eval()\n","    preds = model(img)\n","    for id in range(len(preds)) :\n","        idx_list = []\n","\n","        for idx, score in enumerate(preds[id]['scores']) :\n","            if score > threshold : \n","                idx_list.append(idx)\n","\n","        preds[id]['boxes'] = preds[id]['boxes'][idx_list].cpu()\n","        preds[id]['labels'] = preds[id]['labels'][idx_list].cpu()\n","        preds[id]['scores'] = preds[id]['scores'][idx_list].cpu()\n","\n","    return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3zUpRPGIr48"},"outputs":[],"source":["with torch.no_grad(): \n","    # 테스트셋 배치사이즈= 2\n","    for imgs, annotations in test_data_loader:\n","        imgs = list(img.to(device) for img in imgs)\n","\n","    for img in imgs:\n","        pred.append(make_prediction(model, imgs, 0.1))\n","        print(pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T20:10:14.018723Z","iopub.status.busy":"2022-08-18T20:10:14.018328Z","iopub.status.idle":"2022-08-18T20:10:14.773458Z","shell.execute_reply":"2022-08-18T20:10:14.770732Z","shell.execute_reply.started":"2022-08-18T20:10:14.018672Z"},"id":"myMIN_clIr48","trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","labels = []\n","preds_adj_all = []\n","annot_all = []\n","\n","for im, target in tqdm(data_loader_test, leave=True):\n","    im = list(img.to(device) for img in im)\n","    \n","    for t in target:\n","        labels += t['labels']\n","        \n","    with torch.no_grad():\n","        preds_adj = make_prediction(model, img, 0.5)\n","        preds_adj = [{k: v.to(torch.device('cpu')) for k,v in t.item()} for t in preds_adj]\n","        preds_adj_all.append(preds_adj)\n","        annot_all.append(annot)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRvMriczIr48"},"outputs":[],"source":["import utils_ObjectDetection as utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W12Y81-pIr48"},"outputs":[],"source":["sample_metrics = []\n","for batch_i in range(len(preds_adj_all)):\n","    sample_metrics += utils.get_batch_statistics(preds_adj_all[batch_i], annot_all[batch_i], iou_threshold=0.5)\n","    \n","true_positives, pred_scores, pred_labels = [torch.cat(x.0) for x in list(zip(*sample_metrics))]  # 배치가 전부 합쳐짐\n","precision, recall, AP, f1, ap_class = utils.ap_per_class(true_positives, pred_scores, pred_labels. torch.tensor(labels))\n","mAP = torch.mean(AP)\n","print(f'mAP : {mAP}')\n","print(f'AP : {AP}')\n","print(f'precision : {precision}')\n","print(f'recall : {recall}')\n","print(f'f1 : {f1}')\n","print(f'ap_class : {ap_class}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8C-2_BJIr48"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:15:03.723795Z","iopub.status.busy":"2022-08-18T18:15:03.723399Z","iopub.status.idle":"2022-08-18T18:15:04.305118Z","shell.execute_reply":"2022-08-18T18:15:04.304043Z","shell.execute_reply.started":"2022-08-18T18:15:03.723761Z"},"id":"wrgAjEmaIr48","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[260]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:08:47.287627Z","iopub.status.busy":"2022-08-18T18:08:47.286686Z","iopub.status.idle":"2022-08-18T18:08:47.528515Z","shell.execute_reply":"2022-08-18T18:08:47.527628Z","shell.execute_reply.started":"2022-08-18T18:08:47.287576Z"},"id":"JzlgWSHIuS1R","trusted":true},"outputs":[],"source":["print('EXPECTED OUTPUT')\n","plot_img_bbox(torch_to_pil(img), target, _)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:07:05.060874Z","iopub.status.busy":"2022-08-18T18:07:05.059835Z","iopub.status.idle":"2022-08-18T18:07:05.305674Z","shell.execute_reply":"2022-08-18T18:07:05.303003Z","shell.execute_reply.started":"2022-08-18T18:07:05.060828Z"},"id":"wb80NGCMuY5B","trusted":true},"outputs":[],"source":["print('MODEL OUTPUT')\n","plot_img_bbox(torch_to_pil(img), prediction, _)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:07:08.238440Z","iopub.status.busy":"2022-08-18T18:07:08.237504Z","iopub.status.idle":"2022-08-18T18:07:08.496536Z","shell.execute_reply":"2022-08-18T18:07:08.495620Z","shell.execute_reply.started":"2022-08-18T18:07:08.238400Z"},"id":"uE4VaVMxublo","trusted":true},"outputs":[],"source":["nms_prediction = apply_nms(prediction, iou_thresh=0.2)\n","print('NMS APPLIED MODEL OUTPUT')\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:19:33.513926Z","iopub.status.busy":"2022-08-18T18:19:33.513430Z","iopub.status.idle":"2022-08-18T18:19:34.092269Z","shell.execute_reply":"2022-08-18T18:19:34.091368Z","shell.execute_reply.started":"2022-08-18T18:19:33.513889Z"},"id":"KG88Ck7vIr49","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[369]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:07:12.396891Z","iopub.status.busy":"2022-08-18T18:07:12.396498Z","iopub.status.idle":"2022-08-18T18:07:13.201191Z","shell.execute_reply":"2022-08-18T18:07:13.199911Z","shell.execute_reply.started":"2022-08-18T18:07:12.396858Z"},"id":"NKki3NbxueuR","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[10]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:24:57.606744Z","iopub.status.busy":"2022-08-18T18:24:57.606050Z","iopub.status.idle":"2022-08-18T18:24:58.158487Z","shell.execute_reply":"2022-08-18T18:24:58.155987Z","shell.execute_reply.started":"2022-08-18T18:24:57.606683Z"},"id":"mWjCoJgGIr49","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[1355]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:08:58.574466Z","iopub.status.busy":"2022-08-18T18:08:58.574082Z","iopub.status.idle":"2022-08-18T18:08:59.117513Z","shell.execute_reply":"2022-08-18T18:08:59.115145Z","shell.execute_reply.started":"2022-08-18T18:08:58.574432Z"},"id":"-MVTQ7qyug1J","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[100]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:24:05.219588Z","iopub.status.busy":"2022-08-18T18:24:05.218516Z","iopub.status.idle":"2022-08-18T18:24:05.805594Z","shell.execute_reply":"2022-08-18T18:24:05.804618Z","shell.execute_reply.started":"2022-08-18T18:24:05.219538Z"},"id":"uogbfnNlIr49","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[220]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:16:54.516158Z","iopub.status.busy":"2022-08-18T18:16:54.515780Z","iopub.status.idle":"2022-08-18T18:16:55.115232Z","shell.execute_reply":"2022-08-18T18:16:55.114315Z","shell.execute_reply.started":"2022-08-18T18:16:54.516124Z"},"id":"7SgK9P0PIr49","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[130]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:20:11.132332Z","iopub.status.busy":"2022-08-18T18:20:11.131616Z","iopub.status.idle":"2022-08-18T18:20:11.652522Z","shell.execute_reply":"2022-08-18T18:20:11.651620Z","shell.execute_reply.started":"2022-08-18T18:20:11.132291Z"},"id":"v5tSa6VqIr49","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[321]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:13:59.016387Z","iopub.status.busy":"2022-08-18T18:13:59.015275Z","iopub.status.idle":"2022-08-18T18:13:59.586657Z","shell.execute_reply":"2022-08-18T18:13:59.585650Z","shell.execute_reply.started":"2022-08-18T18:13:59.016338Z"},"id":"ixrSuWg4Ir4-","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[450]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:10:04.559323Z","iopub.status.busy":"2022-08-18T18:10:04.558591Z","iopub.status.idle":"2022-08-18T18:10:05.133452Z","shell.execute_reply":"2022-08-18T18:10:05.132598Z","shell.execute_reply.started":"2022-08-18T18:10:04.559283Z"},"id":"n_bXeCTWIr4-","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[300]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:22:02.708631Z","iopub.status.busy":"2022-08-18T18:22:02.707264Z","iopub.status.idle":"2022-08-18T18:22:03.281306Z","shell.execute_reply":"2022-08-18T18:22:03.279137Z","shell.execute_reply.started":"2022-08-18T18:22:02.708586Z"},"id":"PlQpnOzAIr4-","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[650]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZKYC3eaIr4-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T22:02:39.398114Z","iopub.status.busy":"2022-08-18T22:02:39.397243Z","iopub.status.idle":"2022-08-18T22:02:39.478071Z","shell.execute_reply":"2022-08-18T22:02:39.476660Z","shell.execute_reply.started":"2022-08-18T22:02:39.398019Z"},"id":"WYWTT38_Ir4-","trusted":true},"outputs":[],"source":["test_dataset = PillImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n","# pick one image from the test set\n","img, target = test_dataset[1250]\n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])[0]\n","    \n","print('EXPECTED OUTPUT\\n')\n","plot_img_bbox(torch_to_pil(img), target, target['labels'])\n","print('MODEL OUTPUT\\n')\n","nms_prediction = apply_nms(prediction, iou_thresh=0.01)\n","\n","plot_img_bbox(torch_to_pil(img), nms_prediction, target['labels'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T18:25:59.360518Z","iopub.status.busy":"2022-08-18T18:25:59.359888Z","iopub.status.idle":"2022-08-18T18:25:59.366788Z","shell.execute_reply":"2022-08-18T18:25:59.365452Z","shell.execute_reply.started":"2022-08-18T18:25:59.360480Z"},"id":"8peMFAnYIr4-","trusted":true},"outputs":[],"source":["# %tensorboard --logdir logs/tensorboad"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-08-18T19:20:51.027530Z","iopub.status.busy":"2022-08-18T19:20:51.027167Z","iopub.status.idle":"2022-08-18T19:20:51.034635Z","shell.execute_reply":"2022-08-18T19:20:51.033442Z","shell.execute_reply.started":"2022-08-18T19:20:51.027497Z"},"id":"uI9X811rIr4-","trusted":true},"outputs":[],"source":["import PIL\n","import torchvision.transforms as transforms\n","def imagecrop(img, pred, idx):\n","    xmin, ymin, xmax, ymax = prediction['boxes'].numpy()[0]\n","    \n","    img_t = img\n","    \n","    tf = transforms.ToPILImage()\n","    img_t = tf(img_t)\n","    \n","    image_crop = img_t.crop([xmin, ymin, xmax, ymax])\n","    image_crop.save('/cropimage/'+str(idx)+'.png')\n","    \n","    plt.imshow(image_crop)\n","    plt.show()\n","    \n","    return image_crop\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEzi0d0HIr4-"},"outputs":[],"source":["iterA = iter(data_loader_test)"]}],"metadata":{"colab":{"name":"그게뭐약_최종1_클래스_ALL.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"408008e5ef94774309615aa25401581e8dcb195f3e7f8d5b866d0f3c52da6336"}}},"nbformat":4,"nbformat_minor":0}
